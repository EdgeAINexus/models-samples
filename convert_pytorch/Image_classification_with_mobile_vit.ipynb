{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2024 The AI Edge Torch Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ],
      "metadata": {
        "id": "r4lisalb-A5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This demo will teach you how to convert a PyTorch [MobileViT](https://huggingface.co/docs/transformers/en/model_doc/mobilevit#overview) model to a TensorFlow Lite model intended to run with [MediaPipe](https://developers.google.com/mediapipe/solutions) Tasks using Google's AI Edge Torch library. You will then run the newly converted `tflite` model locally using the MediaPipe Tasks on-device inference tool, as well as learn where to find other tools for running your newly converted model on other edge hardware, including mobile devices and web browsers."
      ],
      "metadata": {
        "id": "LwrH6f2sGJ6U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prerequisites"
      ],
      "metadata": {
        "id": "Mzf2MdHoG-9c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can start by importing the necessary dependencies for converting the model, as well as some additional utilities for displaying various information as you progress through this sample."
      ],
      "metadata": {
        "id": "hux_Gsc_G4nl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r https://github.com/google-ai-edge/ai-edge-torch/releases/download/v0.1.1/requirements.txt\n",
        "!pip install mediapipe\n",
        "!pip install ai-edge-torch==0.1.1"
      ],
      "metadata": {
        "id": "l-9--DWON236"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will also need to download an image to verify model functionality."
      ],
      "metadata": {
        "id": "IUMh9GRk17fV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "\n",
        "IMAGE_FILENAMES = ['cat.jpg']\n",
        "\n",
        "for name in IMAGE_FILENAMES:\n",
        "  url = f'https://storage.googleapis.com/ai-edge/models-samples/torch_converter/image_classification_mobile_vit/{name}'\n",
        "  urllib.request.urlretrieve(url, name)"
      ],
      "metadata": {
        "id": "lfdgp-4Id51J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optionally, you can upload your own image. If you want to do so, uncomment and run the cell below."
      ],
      "metadata": {
        "id": "XYQeTVp-qqZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "\n",
        "# for filename in uploaded:\n",
        "#   content = uploaded[filename]\n",
        "#   with open(filename, 'wb') as f:\n",
        "#     f.write(content)\n",
        "# IMAGE_FILENAMES = list(uploaded.keys())\n",
        "\n",
        "# print('Uploaded files:', IMAGE_FILENAMES)"
      ],
      "metadata": {
        "id": "8X6tiqVGqq0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now go ahead and verify that the image was loaded successfully"
      ],
      "metadata": {
        "id": "RIGfyYzcVkIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import math\n",
        "\n",
        "DESIRED_HEIGHT = 480\n",
        "DESIRED_WIDTH = 480\n",
        "\n",
        "def resize_and_show(image):\n",
        "  h, w = image.shape[:2]\n",
        "  if h < w:\n",
        "    img = cv2.resize(image, (DESIRED_WIDTH, math.floor(h/(w/DESIRED_WIDTH))))\n",
        "  else:\n",
        "    img = cv2.resize(image, (math.floor(w/(h/DESIRED_HEIGHT)), DESIRED_HEIGHT))\n",
        "  cv2_imshow(img)\n",
        "\n",
        "\n",
        "# Preview the images.\n",
        "images = {name: cv2.imread(name) for name in IMAGE_FILENAMES}\n",
        "\n",
        "for name, image in images.items():\n",
        "  print(name)\n",
        "  resize_and_show(image)"
      ],
      "metadata": {
        "id": "-GMYmZ5jVq6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch model validation"
      ],
      "metadata": {
        "id": "IBFYQIm-yFz1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you have your test images, it's time to validate the PyTorch model (in this case MobileViT) that will be converted to the TensorFlow Lite format.\n",
        "\n",
        "Start by retrieving the PyTorch model and the appropriate corresponding processor."
      ],
      "metadata": {
        "id": "g7qbJRCcvQJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import MobileViTImageProcessor, MobileViTForImageClassification\n",
        "\n",
        "hf_model_path = 'apple/mobilevit-small'\n",
        "processor = MobileViTImageProcessor.from_pretrained(hf_model_path)\n",
        "pt_model = MobileViTForImageClassification.from_pretrained(hf_model_path)"
      ],
      "metadata": {
        "id": "flLiQaaL6tU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The MobileViTImageProcessor defined below will perform multiple steps on the input image to match the requirements of the MobileViT model:\n",
        "\n",
        "1. Convert the image from RGB to BGR.\n",
        "2. Rescale the image from the [0, 255] range to the [0, 1] range.\n",
        "3. Resize input image to 256x256 pixels. Differes from default behaviour of processor (includes padding and center cropping) to make it easier to validate the converted model with MediaPipe Tasks (more details in the corresponding section)."
      ],
      "metadata": {
        "id": "4Mik12qkNEU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "images = []\n",
        "for filename in IMAGE_FILENAMES:\n",
        "  images.append(Image.open(filename))\n",
        "\n",
        "inputs = processor(\n",
        "    images=images,\n",
        "    return_tensors='pt',\n",
        "    size={'height': 256, 'width': 256},\n",
        "    do_center_crop=False\n",
        ")"
      ],
      "metadata": {
        "id": "_-WmB2MYWc-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you have your test data ready and the inputs processed, it's time to validate the classifications. In this step you will loop through your test image(s) and display the top 5 predicted classification categories. This model was trained with ImageNet-1000, so there are 1000 different potential classifications that may be applied to your test data."
      ],
      "metadata": {
        "id": "ZAQG5SSVzVi2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "for image_index in range(len(IMAGE_FILENAMES)) :\n",
        "  outputs = pt_model(**inputs)\n",
        "  logits = outputs.logits\n",
        "  probs, indices = nn.functional.softmax(logits[image_index], dim=-1).flatten().topk(k=5)\n",
        "\n",
        "  print(IMAGE_FILENAMES[image_index], 'predictions: ')\n",
        "  for prediction_index in range(len(indices)):\n",
        "    class_label = pt_model.config.id2label[indices[prediction_index].item()]\n",
        "    prob = probs[prediction_index].item()\n",
        "    print(f'{(prob * 100):4.1f}%  {class_label}')\n",
        "  print('\\n')"
      ],
      "metadata": {
        "id": "ofbZW6nVzSrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert to TFLite"
      ],
      "metadata": {
        "id": "pfJkS3bH7Jpw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before converting the PyTorch model to TFLite, you will need to take an extra step to match it to the format expected by MediaPipe (MP) Tasks. Here are the necessary adjustments:\n",
        "\n",
        "1. MediaPipe Tasks require channel-last images (BHWC) while PyTorch uses channel-first (BCHW).\n",
        "\n",
        "2. For the Image Classification Task, MediaPipe requires an additional sigmoid layer on classification logits.\n",
        "\n",
        "You can also include preprocessing steps into a wrapper, such as converting from RGB to BGR and scaling, similar to what you did when validating the PyTorch model in the previous section."
      ],
      "metadata": {
        "id": "ci-8lp_55TLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HF2MP_ImageClassificationModelWrapper(nn.Module):\n",
        "\n",
        "  def __init__(self, hf_image_classification_model, hf_processor):\n",
        "    super().__init__()\n",
        "    self.model = hf_image_classification_model\n",
        "    if hf_processor.do_rescale:\n",
        "      self.rescale_factor = hf_processor.rescale_factor\n",
        "    else:\n",
        "      self.rescale_factor = 1.0\n",
        "\n",
        "  def forward(self, image: torch.Tensor):\n",
        "    # BHWC -> BCHW.\n",
        "    image = image.permute(0, 3, 1, 2)\n",
        "    # RGB -> BGR.\n",
        "    image = image.flip(dims=(1,))\n",
        "    # Scale [0, 255] -> [0, 1].\n",
        "    image = image * self.rescale_factor\n",
        "    logits = self.model(pixel_values=image).logits  # [B, 1000] float32.\n",
        "    # Softmax is required for MediaPipe classification model.\n",
        "    logits = torch.nn.functional.softmax(logits, dim=-1)\n",
        "\n",
        "    return logits\n",
        "\n",
        "\n",
        "hf_model_path = 'apple/mobilevit-small'\n",
        "hf_mobile_vit_processor = MobileViTImageProcessor.from_pretrained(hf_model_path)\n",
        "hf_mobile_vit_model = MobileViTForImageClassification.from_pretrained(hf_model_path)\n",
        "wrapped_pt_model = HF2MP_ImageClassificationModelWrapper(\n",
        "hf_mobile_vit_model, hf_mobile_vit_processor).eval()"
      ],
      "metadata": {
        "id": "NlBmvShe4Mt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert to TFLite"
      ],
      "metadata": {
        "id": "GMBNfgcV7k0f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it's time to perform the conversion! You will need to provide simple arguments, such as the expected input shape (in this case three layers for images that are 256 height by 256 width)."
      ],
      "metadata": {
        "id": "T2MnULes70W0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ai_edge_torch\n",
        "\n",
        "sample_args = (torch.rand((1, 256, 256, 3)),)\n",
        "edge_model = ai_edge_torch.convert(wrapped_pt_model, sample_args)"
      ],
      "metadata": {
        "id": "XOfNPYpnLGrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the conversion is finished and you have a new `tflite` model file, you will convert the raw tflite file into a *model buffer* so that you can do a little more additional processing to prepare the file for working with MediaPipe Tasks. This includes attaching the labels for the model to the new `tflite` model so that it can be used with MediaPipe Tasks Image Classification."
      ],
      "metadata": {
        "id": "HPbeMLwbLZb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mediapipe.tasks.python.metadata.metadata_writers import image_classifier\n",
        "from mediapipe.tasks.python.metadata.metadata_writers import metadata_writer\n",
        "from mediapipe.tasks.python.vision.image_classifier import ImageClassifier\n",
        "from pathlib import Path\n",
        "\n",
        "flatbuffer_file = Path('hf_mobile_vit_mp_image_classification_raw.tflite')\n",
        "edge_model.export(flatbuffer_file)\n",
        "tflite_model_buffer = flatbuffer_file.read_bytes()\n",
        "\n",
        "labels = list(hf_mobile_vit_model.config.id2label.values())\n",
        "\n",
        "writer = image_classifier.MetadataWriter.create(\n",
        "    tflite_model_buffer,\n",
        "    input_norm_mean=[0.0], #  Normalization is not needed for this model.\n",
        "    input_norm_std=[1.0],\n",
        "    labels=metadata_writer.Labels().add(labels),\n",
        ")\n",
        "tflite_model_buffer, _ = writer.populate()"
      ],
      "metadata": {
        "id": "1mDOCFdG7H16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After attaching the metadata to the intermediate model buffer object, you can convert the buffer back into a `tflite` file."
      ],
      "metadata": {
        "id": "AMk6rSsfDykf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tflite_filename = 'hf_mobile_vit_mp_image_classification.tflite'\n",
        "# Save converted model to Colab's local file system.\n",
        "with open(tflite_filename, 'wb') as f:\n",
        "  f.write(tflite_model_buffer)"
      ],
      "metadata": {
        "id": "jpQ8R2pxQrIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before moving on to *using* the converted model, it's always a good idea to make sure the model was successefully saved."
      ],
      "metadata": {
        "id": "1KVR8e4V8pou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l /content/hf_mobile_vit_mp_image_classification.tflite"
      ],
      "metadata": {
        "id": "wuwP7uMzCAS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validate converted model with MediaPipe Tasks"
      ],
      "metadata": {
        "id": "e7II2a_389DH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it's time to test your newly converted model directly with the MediaPipe Image Classification Task. Before getting into that code, you can add the following utility functions to improve the output displayed."
      ],
      "metadata": {
        "id": "-3kFtIGK_1qi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.rcParams.update({\n",
        "    'axes.spines.top': False,\n",
        "    'axes.spines.right': False,\n",
        "    'axes.spines.left': False,\n",
        "    'axes.spines.bottom': False,\n",
        "    'xtick.labelbottom': False,\n",
        "    'xtick.bottom': False,\n",
        "    'ytick.labelleft': False,\n",
        "    'ytick.left': False,\n",
        "    'xtick.labeltop': False,\n",
        "    'xtick.top': False,\n",
        "    'ytick.labelright': False,\n",
        "    'ytick.right': False\n",
        "})\n",
        "\n",
        "\n",
        "def display_one_image(image, title, subplot, titlesize=16):\n",
        "    \"\"\"Displays one image along with the predicted category name and score.\"\"\"\n",
        "    plt.subplot(*subplot)\n",
        "    plt.imshow(image)\n",
        "    if len(title) > 0:\n",
        "        plt.title(title, fontsize=int(titlesize), color='black', fontdict={'verticalalignment':'center'}, pad=int(titlesize/1.5))\n",
        "    return (subplot[0], subplot[1], subplot[2]+1)\n",
        "\n",
        "def display_batch_of_images(images, predictions):\n",
        "    \"\"\"Displays a batch of images with the classifications.\"\"\"\n",
        "    # Images and predictions.\n",
        "    images = [image.numpy_view() for image in images]\n",
        "\n",
        "    # Auto-squaring: this will drop data that does not fit into square or square-ish rectangle.\n",
        "    rows = int(math.sqrt(len(images)))\n",
        "    cols = len(images) // rows\n",
        "\n",
        "    # Size and spacing.\n",
        "    FIGSIZE = 13.0\n",
        "    SPACING = 0.1\n",
        "    subplot=(rows,cols, 1)\n",
        "    if rows < cols:\n",
        "        plt.figure(figsize=(FIGSIZE,FIGSIZE/cols*rows))\n",
        "    else:\n",
        "        plt.figure(figsize=(FIGSIZE/rows*cols,FIGSIZE))\n",
        "\n",
        "    # Display.\n",
        "    for i, (image, prediction) in enumerate(zip(images[:rows*cols], predictions[:rows*cols])):\n",
        "        dynamic_titlesize = FIGSIZE * SPACING / max(rows,cols) * 40 + 3\n",
        "        subplot = display_one_image(image, prediction, subplot, titlesize=dynamic_titlesize)\n",
        "\n",
        "    # Layout.\n",
        "    plt.tight_layout()\n",
        "    plt.subplots_adjust(wspace=SPACING, hspace=SPACING)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "mV2Uo2yg2Lw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it's time to move on to the actual inference code and display the highest confidence classification result.\n",
        "\n",
        "While the converted model expects a square input image with a height of 256 pixels and a width of 256 pixels, the MediaPipe Image Classification Task automatically resizes and adds padding to the input image to meet the model's input requirements.\n",
        "\n",
        "During this validation step, you will ensure that the converted model produces roughly the same output as the original PyTorch model for the same input. One thing worth noting is since the resizing and padding in MediaPipe differs from that performed in MobileViTImageProcessor, there will likely be some minor differences in prediction confidences. To account for this, we will bypass the padding and automatic resizing step by resizing the input image manually before feeding it to the image classifier."
      ],
      "metadata": {
        "id": "GY-UcEls4TH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mediapipe as mp\n",
        "from mediapipe.tasks import python\n",
        "from mediapipe.tasks.python.components import processors\n",
        "from mediapipe.tasks.python import vision\n",
        "\n",
        "# STEP 1: Create an ImageClassifier object.\n",
        "\n",
        "base_options= python.BaseOptions(\n",
        "        model_asset_path=f'/content/{tflite_filename}')\n",
        "\n",
        "options = vision.ImageClassifierOptions(\n",
        "    base_options=base_options,\n",
        "    max_results=5)\n",
        "\n",
        "classifier = vision.ImageClassifier.create_from_options(options)\n",
        "\n",
        "images = []\n",
        "predictions = []\n",
        "for image_name in IMAGE_FILENAMES:\n",
        "  # STEP 2: Load the input image(s).\n",
        "  image = mp.Image.create_from_file(image_name)\n",
        "\n",
        "  # STEP 3: Classify the input image(s).\n",
        "  classification_result = classifier.classify(image)\n",
        "\n",
        "  # STEP 4: Process the classification result. In this case, visualize it.\n",
        "  images.append(image)\n",
        "  top_category = classification_result.classifications[0].categories[0]\n",
        "  predictions.append(f\"{top_category.category_name} ({top_category.score:.2f})\")\n",
        "\n",
        "display_batch_of_images(images, predictions)"
      ],
      "metadata": {
        "id": "4GdTsCQP10To"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should now see your loaded test images and their confidence scores/classifications that match the original PyTorch model results! If everything looks good, the final step should be downloading your newly converted `tflite` model file to your computer so you can use it elsewhere."
      ],
      "metadata": {
        "id": "1bxosFdH_99n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download(tflite_filename)"
      ],
      "metadata": {
        "id": "mY00XJQ1BZP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Next steps\n",
        "\n",
        "Now that you have learned how to convert a PyTorch model to the TFLite format, it's time to do more with it! You can go over additional [MediaPipe](https://github.com/google-ai-edge/mediapipe-samples) samples for Android, iOS, web, and Python (including the Raspberry Pi!) to try your new model on multiple platforms, check out the [TFLite Interpreter API](https://ai.google.dev/edge/lite/) for running custom solutions, and read more about the PyTorch to TFLite framework with our [official documentation](https://ai.google.dev/edge/lite/models/convert_pytorch)."
      ],
      "metadata": {
        "id": "3AOmkXUaBVUb"
      }
    }
  ]
}
